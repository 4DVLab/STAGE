<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <!-- <meta name="description" content="DESCRIPTION META TAG"> -->

    <meta property='og:title' content='STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation'/>
    <meta property='og:url' content='https://4DVlab.github.io/STAGE-page/'/>
    <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
    <!-- <meta name="twitter:card" content="summary_large_image"> -->
    <!-- Keywords for your paper to be indexed by-->
    <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>STAGE</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/tab_gallery.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
    <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/magnifier.js"></script>
    <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/image_card_fader.css">
    <link rel="stylesheet" href="./static/css/image_card_slider.css">
</head>

<body>
  <section class="hero banner">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Jiamin Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Yichen Yao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Xiang Feng<sup>1</sup>,
            </span>
            <span class="author-block">
              Hang Wu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              Yaming Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              Qingqiu Huang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              Yuexin Ma</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Xinge Zhu</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ShanghaiTech University,</span>
            <span class="author-block"><sup>2</sup>Yinwang Intelligent Technology</span>
          </div>

          <div class="is-size-5 publication-venue">
            in IROS 2025(Oral)
          </div>

          <!-- Paper pdf. -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="static/paper/STAGE_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              

              <!-- Arxiv 部分. -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2506.13138"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Code Link.
              <span class="link-block">
                <a href="https://github.com/4DVLab/UniDemoire"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>UniDemoiré</span>
                  </a>
              </span> -->


              <!-- <span class="link-block">
                <a href="static/slides/[PPT]Diffusion2GAN.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span>


              <span class="link-block">
                <a href="static/slides/[poster]Diffusion2GAN.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span> -->

              <!-- 引用.
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span>  -->


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-4">Two-step 4K image generation using Diffusion2GAN and GigaGAN</h2>
      <div class="content has-text-justified">
        <p>
          <b>The images generated by Diffusion2GAN can be seamlessly upsampled to 4k res using GigaGAN upsampler.</b> This indicates that we can generate low-resolution preview images using Diffusion2GAN and then enhance some preferred images to 4k resolution using the GigaGAN upsampler.
          <br> 
        </p>
      </div>

      <div class="tab_container">
       
        <div id="juxtapose-embed" data-startingposition="30%" data-animate="true">
        </div>

        <div>
          <div id="juxtapose-hidden"></div>
        </div>
        
        <div id="imgtext"></div>
      </div>

      <div class="tab_row">
        <div class="tab_column">
          <img src="./static/images/1_input.jpeg"  onclick="tab_gallery_click('1');">
        </div>

        <div class="tab_column">
          <img src="./static/images/3_input.jpeg" onclick="tab_gallery_click('3');">
        </div>
        <div class="tab_column">
          <img src="./static/images/4_input.jpeg" onclick="tab_gallery_click('4');">
        </div>
        <div class="tab_column">
          <img src="./static/images/5_input.jpeg" onclick="tab_gallery_click('5');">
        </div>
        <div class="tab_column">
          <img src="./static/images/6_input.jpeg" onclick="tab_gallery_click('6');">
        </div>
        
      </div>
    </div>
  </div>

</section> -->



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The generation of temporally consistent, high-fidelity driving videos over extended horizons presents a fundamental challenge in autonomous driving world modeling. Existing approaches often suffer from error accumulation and feature misalignment due to inadequate decoupling of spatio-temporal dynamics and limited cross-frame feature propagation mechanisms. To address these limitations, we present STAGE (Streaming Temporal Attention Generative Engine), a novel auto-regressive framework that pioneers hierarchical feature coordination and multi-phase optimization for sustainable video synthesis.
            To achieve high-quality long-horizon driving video generation, we introduce Hierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training strategy. HTFT enhances temporal consistency between video frames throughout the video generation process by modeling the temporal and denoising process separately and transferring denoising features between frames. The multi-stage training strategy is to divide the training into three stages, through model decoupling and auto-regressive inference process simulation, thereby accelerating model convergence and reducing error accumulation.
            Experiments on the Nuscenes dataset show that STAGE has significantly surpassed existing methods in the long-horizon driving video generation task. In addition, we also explored STAGE's ability to generate unlimited-length driving videos. We generated 600 frames of high-quality driving videos on the Nuscenes dataset, which far exceeds the maximum length achievable by existing methods.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Method</h2>

        <!-- 总体 Pipeline 图 -->
        <div class="content has-text-justified">
          <img src="./static/images/Pipeline.png" width="100%">
          <p>
            <b>Overview of STAGE</b>.“AF”, “CF”, “NF” stand for Anchor Frame, Condition Frame, Noise Frame, respectively. (I) illustrates the hierarchical
            structuring of time and denoising steps, with the horizontal axis representing time and the vertical axis representing the denoising steps. T represents the
            T-th frame, while t refers to the t-th denoising step. (II) illustrates the framework of our model, where we leverage HTFT to facilitate feature transfer
            along the temporal dimension, thereby refining the generation process at each step. (III) presents our multi-stage training strategy and the process for
            infinite generation.
          </p>
          <br>
        </div>

        <!-- <div class="content has-text-justified">
          <p>
            
          </p>
          <br>
        </div> -->
 
        <!-- 1. Moire Patterns Collection -->
        <h3 class="title is-4">Visualization</h3>
        
        <div class="content has-text-justified">
          <p>
            Qualitative Comparison between Vista and STAGE in long video generation task. We generated 201 frames and selected frames 41, 81, 121,
            161, and 201 for the comparison.
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="static\images\stage3_long_merge_0000064_fix_ci.jpg" width="100%">

        </div>

        <div class="content has-text-justified">
          <p>
            Qualitative Comparison between Vista and STAGE in short video generation task. We generated 16 frames, and selected frames 2, 5, 9,
            and 16 for comparison.
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="static\images\stage3_short_merge_0000044_fix_ci.jpg" width="100%">
          <p>
            Visualization of Longer Video Generation. We generate 601 frames and selected frames 121, 241, 361, 481, and 601 for the visualization.
          </p>
        </div>


        <div class="content has-text-centered">
            <img src="static\images\600_frames.jpg" width="90%">
            <p>
              Visualization of Longer Video Generation. We generate 601 frames and selected frames 121, 241, 361, 481, and 601 for the visualization.
            </p>
        </div>

        <!-- <div class="content has-text-justified">
          <p>
            Data collection setup (left), 
            and examples of moiré patterns in our dataset captured at different zoom rates and screen panel (middle), and our generated patterns (right).
          </p>
        </div> -->
        <h3 class="title is-4">Video</h3>
        <!-- <div class="content has-text-justified">
            <p>240 frames generated by STAGE.</p>
        </div> -->
        <div class="video-grid-container">
            <div class="video-item">
                <video controls muted autoplay loop>
                    <source src="static/videos/stage3_short_5134.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-item">
                <video controls muted autoplay loop>
                    <source src="static/videos/stage3_short_5369.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-item">
                <video controls muted autoplay loop>
                    <source src="static/videos/stage3_short_25163.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-item">
                <video controls muted autoplay loop>
                    <source src="static/videos/stage3_short_26109.mp4" type="video/mp4">

                </video>
            </div>
        </div>
        <div class="content has-text-centered">
            <p>
              240 frames generated by STAGE.
            </p>
            <br>
        </div>
        <style>
            /* 视频容器样式 */
            .video-grid-container {
                display: grid;
                /* 将这里改为4列，每列占据可用空间的均等部分 */
                grid-template-columns: repeat(4, 1fr); 
                /* 将这里改为1行，如果需要保持高度则可不设置 */
                grid-template-rows: 1fr;    
                gap: 10px; /* 视频之间的间距 */
                width: 100%; /* 容器宽度占据父元素（column is-full-width）的100% */
                max-width: 1500px; /* 限制最大宽度 */
                margin: 10px auto; /* 上下外边距20px，左右自动居中 */
                /* 如果所有视频都在一行，可以考虑移除 aspect-ratio，让视频的高度根据宽度和自身比例自适应 */
                /* aspect-ratio: 16 / 9; */ 
                background-color: #333; /* 背景色，方便查看效果 */
                padding: 10px;
                border-radius: 8px;
                box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
            }
        
            /* 单个视频项样式 */
            .video-item {
                position: relative; /* 用于子元素定位 */
                width: 100%;
                height: 0;
                padding-bottom: 56.25%; /* 16:9 视频宽高比 (9 / 16 * 100%) */
                background-color: #000;
                overflow: hidden; /* 隐藏超出部分 */
                border-radius: 4px;
            }
        
            .video-item video {
                position: absolute;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
                object-fit: cover; /* 确保视频填充整个容器，可能裁剪边缘 */
            }
        
            /* 响应式调整 */
            @media (max-width: 768px) {
                .video-grid-container {
                    /* 在小屏幕上仍然可以保持4列，但视频会非常小 */
                    /* 如果希望小屏幕上变为单列，则改为：grid-template-columns: 1fr; */
                    grid-template-columns: repeat(2, 1fr); /* 在小屏幕上可以考虑变为两列，防止过小 */
                    grid-template-rows: auto;
                    aspect-ratio: auto; /* 取消固定宽高比 */
                }
        
                .video-item {
                    padding-bottom: 56.25%; /* 保持视频本身的宽高比 */
                }
            }
        </style>
        <div class="video-grid-container">
          <div class="video-item">
              <video controls muted autoplay loop>
                  <source src="static\videos\stage4_470.mp4" type="video/mp4">
              </video>
          </div>
          <div class="video-item">
              <video controls muted autoplay loop>
                  <source src="static\videos\stage4_14263.mp4" type="video/mp4">
              </video>
          </div>
          <div class="video-item">
              <video controls muted autoplay loop>
                  <source src="static\videos\stage4_25398.mp4" type="video/mp4">
              </video>
          </div>
          <div class="video-item">
              <video controls muted autoplay loop>
                  <source src="static\videos\stage4_2115.mp4" type="video/mp4">

              </video>
          </div>
      </div>
      <div class="content has-text-centered">
          <p>
            600 frames generated by STAGE.
          </p>
          <br>
      </div>
        
      </div>
    </div>
  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>@misc{yang2025unidemoire,
  author    = {Zemin Yang, Yujing Sun, Xidong Peng, Siu Ming Yiu, Yuexin Ma},
  title     = {UniDemoiré: Towards Universal Image Demoiréing with Data Generation and Synthesis},
  year      = {2025},
  eprint    = {2502.06324},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url={https://arxiv.org/abs/2502.06324},
}</code></pre>
  </div>
</section> -->

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code> Coming soon. </code></pre>
  </div>
</section> -->


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>

<script>
var slider;
let origImages = [
  {"src": "./static/images/1_output.jpeg", "label": "Moiré Image(Synthesized by our UniDemoiré)",},
  {"src": "./static/images/1_input.jpeg", "label": "Pure Moiré Pattern",},
  
];
let origOptions = {
    "makeResponsive": true,
    "showLabels": true,
    "mode": "horizontal",
    "showCredits": true,
    "animate": true,
    "startingPosition": "50"
};

const juxtaposeSelector = "#juxtapose-embed";
const transientSelector = "#juxtapose-hidden";


// function tab_gallery_click(name) {
//   // Get the expanded image
//   let inputImage = {
//     label: "Clean Image",
//   };
//   let outputImage = {
//     label: "Pure Moiré Pattern (Synthesized by our UniDemoiré)",
//   };

//   inputImage.src = "./static/images/".concat(name, "_input.jpeg")
//   outputImage.src = "./static/images/".concat(name, "_output.jpeg")

//   // let images = [inputImage, outputImage];
//   let images = [outputImage, inputImage];
//   let options = slider.options;
//   options.callback = function(obj) {
//       var newNode = document.getElementById(obj.selector.substring(1));
//       var oldNode = document.getElementById(juxtaposeSelector.substring(1));
//       console.log(obj.selector.substring(1));
//       console.log(newNode.children[0]);
//       oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
//       //newNode.removeChild(newNode.children[0]);
      
//   };
  
//   slider = new juxtapose.JXSlider(transientSelector, images, options);
// };



(function() {
    slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
    //document.getElementById("left-button").onclick = replaceLeft;
    //document.getElementById("right-button").onclick = replaceRight;
})();
  // Get the image text
  var imgText = document.getElementById("imgtext");
  // Use the same src in the expanded image as the image being clicked on from the grid
  // expandImg.src = imgs.src;
  // Use the value of the alt attribute of the clickable image as text inside the expanded image
  imgText.innerHTML = name;
  // Show the container element (hidden with CSS)
  // expandImg.parentElement.style.display = "block";

$(".flip-card").click(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("out");
            $(div_front).removeClass("in");

            $(div_back).addClass("in");
            $(div_back).removeClass("out");

});

$(".flip-card").mouseleave(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("in");
            $(div_front).removeClass("out");

            $(div_back).addClass("out");
            $(div_back).removeClass("in");

});

</script>
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
