<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <!-- <meta name="description" content="DESCRIPTION META TAG"> -->

    <meta property='og:title' content='STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation'/>
    <meta property='og:url' content='https://4DVlab.github.io/STAGE-page/'/>
    <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
    <!-- <meta name="twitter:card" content="summary_large_image"> -->
    <!-- Keywords for your paper to be indexed by-->
    <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>STAGE</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/tab_gallery.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
    <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/magnifier.js"></script>
    <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/image_card_fader.css">
    <link rel="stylesheet" href="./static/css/image_card_slider.css">
</head>

<body>
  <section class="hero banner">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Jiamin Wang</a><sup>1,*</sup>,
            </span>
            <span class="author-block">
              Yichen Yao</a><sup>1,*</sup>,
            </span>
            <span class="author-block">
              Xiang Feng<sup>1</sup>,
            </span>
            <span class="author-block">
              Hang Wu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              Yaming Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              Qingqiu Huang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              Yuexin Ma</a><sup>1,&dagger;</sup>,
            </span>
            <span class="author-block">
              Xinge Zhu</a><sup>2,&dagger;</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ShanghaiTech University,</span>
            <span class="author-block"><sup>2</sup>Yinwang Intelligent Technology</span>
          </div>

          <div class="is-size-5 publication-venue">
            in IROS 2025
          </div>

          <!-- Paper pdf. -->
          <div class="column has-text-centered">
            <div class="publication-links">
      

              <!-- Arxiv 部分. -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2506.13138"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The generation of temporally consistent, high-fidelity driving videos over extended horizons presents a fundamental challenge in autonomous driving world modeling. Existing approaches often suffer from error accumulation and feature misalignment due to inadequate decoupling of spatio-temporal dynamics and limited cross-frame feature propagation mechanisms. To address these limitations, we present STAGE (Streaming Temporal Attention Generative Engine), a novel auto-regressive framework that pioneers hierarchical feature coordination and multi-phase optimization for sustainable video synthesis.
            To achieve high-quality long-horizon driving video generation, we introduce Hierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training strategy. HTFT enhances temporal consistency between video frames throughout the video generation process by modeling the temporal and denoising process separately and transferring denoising features between frames. The multi-stage training strategy is to divide the training into three stages, through model decoupling and auto-regressive inference process simulation, thereby accelerating model convergence and reducing error accumulation.
            Experiments on the Nuscenes dataset show that STAGE has significantly surpassed existing methods in the long-horizon driving video generation task. In addition, we also explored STAGE's ability to generate unlimited-length driving videos. We generated 600 frames of high-quality driving videos on the Nuscenes dataset, which far exceeds the maximum length achievable by existing methods.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Method</h2>

        <!-- 总体 Pipeline 图 -->
        <div class="content has-text-justified">
          <img src="./static/images/Pipeline.png" width="100%">
          <p>
            <b>Overview of STAGE</b>.“AF”, “CF”, “NF” stand for Anchor Frame, Condition Frame, Noise Frame, respectively. (I) illustrates the hierarchical
            structuring of time and denoising steps, with the horizontal axis representing time and the vertical axis representing the denoising steps. T represents the
            T-th frame, while t refers to the t-th denoising step. (II) illustrates the framework of our model, where we leverage HTFT to facilitate feature transfer
            along the temporal dimension, thereby refining the generation process at each step. (III) presents our multi-stage training strategy and the process for
            infinite generation.
          </p>
          <br>
        </div>

        <h2 class="title is-3 has-text-centered">Visualization</h2>
        

        <div class="content has-text-centered">
          <img src="static\images\stage3_long_merge_0000064_fix_ci.jpg" width="100%">
          <p>
            Qualitative Comparison between Vista and STAGE in long video generation task. We generated 201 frames and selected frames 41, 81, 121,
            161, and 201 for the comparison.
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="static\images\stage3_short_merge_0000044_fix_ci.jpg" width="100%">
          <p>
            Qualitative Comparison between Vista and STAGE in short video generation task. We generated 16 frames, and selected frames 2, 5, 9,
            and 16 for comparison.
          </p>
        </div>


        <div class="content has-text-centered">
            <img src="static\images\600_frames.jpg" width="100%">
            <p>
              Visualization of Longer Video Generation. We generate 601 frames and selected frames 121, 241, 361, 481, and 601 for the visualization.
            </p>
        </div>

        <h2 class="title is-3 has-text-centered">Videos</h2>

        <div class="video-grid-container">
            <div class="video-item">
                <video controls muted autoplay loop>
                    <source src="static/videos/stage3_short_5134.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-item">
                <video controls muted autoplay loop>
                    <source src="static/videos/stage3_short_5369.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-item">
                <video controls muted autoplay loop>
                    <source src="static/videos/stage3_short_25163.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-item">
                <video controls muted autoplay loop>
                    <source src="static/videos/stage3_short_26109.mp4" type="video/mp4">

                </video>
            </div>
        </div>
        <div class="content has-text-centered">
            <p>
              240 frames generated by STAGE.
            </p>
            <br>
        </div>
        <style>

            .video-grid-container {
                display: grid;

                grid-template-columns: repeat(4, 1fr); 

                grid-template-rows: 1fr;    
                gap: 10px;
                width: 100%; 
                max-width: 1500px;
                margin: 10px auto;
                background-color: #333; 
                padding: 10px;
                border-radius: 8px;
                box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
            }
        
            .video-item {
                position: relative;
                width: 100%;
                height: 0;
                padding-bottom: 56.25%;
                background-color: #000;
                overflow: hidden; 
                border-radius: 4px;
            }
        
            .video-item video {
                position: absolute;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
                object-fit: cover; 
            }
        
            /* 响应式调整 */
            @media (max-width: 768px) {
                .video-grid-container {
                    grid-template-columns: repeat(2, 1fr); 
                    grid-template-rows: auto;
                    aspect-ratio: auto; 
                }
        
                .video-item {
                    padding-bottom: 56.25%; 
                }
            }
        </style>
        <div class="video-grid-container">
          <div class="video-item">
              <video controls muted autoplay loop>
                  <source src="static\videos\stage4_470.mp4" type="video/mp4">
              </video>
          </div>
          <div class="video-item">
              <video controls muted autoplay loop>
                  <source src="static\videos\stage4_14263.mp4" type="video/mp4">
              </video>
          </div>
          <div class="video-item">
              <video controls muted autoplay loop>
                  <source src="static\videos\stage4_25398.mp4" type="video/mp4">
              </video>
          </div>
          <div class="video-item">
              <video controls muted autoplay loop>
                  <source src="static\videos\stage4_2115.mp4" type="video/mp4">

              </video>
          </div>
      </div>
      <div class="content has-text-centered">
          <p>
            600 frames generated by STAGE.
          </p>
          <br>
      </div>
        
      </div>
    </div>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>

<script>
var slider;
let origImages = [
  {"src": "./static/images/1_output.jpeg", "label": "Moiré Image(Synthesized by our UniDemoiré)",},
  {"src": "./static/images/1_input.jpeg", "label": "Pure Moiré Pattern",},
  
];
let origOptions = {
    "makeResponsive": true,
    "showLabels": true,
    "mode": "horizontal",
    "showCredits": true,
    "animate": true,
    "startingPosition": "50"
};

const juxtaposeSelector = "#juxtapose-embed";
const transientSelector = "#juxtapose-hidden";




(function() {
    slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
})();
  // Get the image text
  var imgText = document.getElementById("imgtext");
  // Use the same src in the expanded image as the image being clicked on from the grid
  // expandImg.src = imgs.src;
  // Use the value of the alt attribute of the clickable image as text inside the expanded image
  imgText.innerHTML = name;
  // Show the container element (hidden with CSS)
  // expandImg.parentElement.style.display = "block";

$(".flip-card").click(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("out");
            $(div_front).removeClass("in");

            $(div_back).addClass("in");
            $(div_back).removeClass("out");

});

$(".flip-card").mouseleave(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("in");
            $(div_front).removeClass("out");

            $(div_back).addClass("out");
            $(div_back).removeClass("in");

});

</script>
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
